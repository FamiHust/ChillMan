{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "language-model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FamiHust/ChillMan/blob/main/language-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwUhQkprdY_e"
      },
      "source": [
        "# **0. Running from Google Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD-idowHa6Dq",
        "outputId": "e05c2eb4-0696-4b31-e374-bba4847cfc45"
      },
      "source": [
        "!git clone https://github.com/hieutrgvu/text-generation-and-correction.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text-generation-and-correction'...\n",
            "remote: Enumerating objects: 1053, done.\u001b[K\n",
            "remote: Counting objects: 100% (1053/1053), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1020/1020), done.\u001b[K\n",
            "remote: Total 1053 (delta 28), reused 1039 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1053/1053), 9.70 MiB | 7.32 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjximMx9b19V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3ee303-8e2e-403b-846a-0072330755d7"
      },
      "source": [
        "cd \"text-generation-and-correction\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-generation-and-correction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F82bwKXpMLvj",
        "outputId": "097e08cd-3baa-4924-c986-a3e9d6b38cab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2tr28OWe6M9"
      },
      "source": [
        "# **1. Import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laZeC49Ce1w7"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from scipy import special"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYASI8qTd04F"
      },
      "source": [
        "# **2. Load, Clean and Augment Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFIzb1jtdCaj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0088f17-54ce-444e-d8f7-22eb5448a050"
      },
      "source": [
        "# load\n",
        "lines = []\n",
        "data_dir = \"./tiki-data\"\n",
        "for file in os.listdir(data_dir):\n",
        "  if file.startswith(\"sach-\"):\n",
        "    with open(data_dir+\"/\"+file) as f:\n",
        "      lines.extend(f.readlines())\n",
        "\n",
        "print(\"Number of lines: \", len(lines))\n",
        "lines[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines:  10079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Nuôi Con Không Phải Là Cuộc Chiến 2 (Trọn Bộ 3 Tập)\\n',\n",
              " 'Để Con Được Ốm (Tái Bản 2018)\\n',\n",
              " 'Combo Nuôi Con Không Phải Là Cuộc Chiến 2 - Tái Bản 2019 (Quyển 1 + 2 + 3) + Nuôi Con Không Phải Là Cuộc Chiến (Bộ 4 Cuốn)\\n',\n",
              " '90% Trẻ Thông Minh Nhờ Cách Trò Chuyện Đúng Đắn Của Cha Mẹ\\n',\n",
              " 'Thai Giáo Theo Chuyên Gia - 280 Ngày - Mỗi Ngày Đọc Một Trang\\n',\n",
              " 'Vô Cùng Tàn Nhẫn Vô Cùng Yêu Thương (Tái Bản 2017)\\n',\n",
              " 'Đọc Vị Mọi Vấn Đề Của Trẻ (Tái Bản 2018)\\n',\n",
              " 'Ăn Dặm Kiểu Nhật (Tái Bản 2018)\\n',\n",
              " 'Người Mẹ Tốt Hơn Là Người Thầy Tốt (Tái Bản 2015)\\n',\n",
              " 'Combo nuôi con không phải cuộc chiến bộ 4 cuốn\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVPM-G5YnTIq",
        "outputId": "7e18ea7c-8572-4644-8d0b-54c47ee4a17b"
      },
      "source": [
        "# clean\n",
        "bos = \"{\"\n",
        "eos = \"}\"\n",
        "regex = \"[^0-9a-zạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ]\"\n",
        "for i in range(len(lines)):\n",
        "  lines[i] = re.sub(regex, \" \", lines[i].lower()).strip()\n",
        "  lines[i] = bos + re.sub(' +', ' ', lines[i])  + eos\n",
        "lines[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{nuôi con không phải là cuộc chiến 2 trọn bộ 3 tập}',\n",
              " '{để con được ốm tái bản 2018}',\n",
              " '{combo nuôi con không phải là cuộc chiến 2 tái bản 2019 quyển 1 2 3 nuôi con không phải là cuộc chiến bộ 4 cuốn}',\n",
              " '{90 trẻ thông minh nhờ cách trò chuyện đúng đắn của cha mẹ}',\n",
              " '{thai giáo theo chuyên gia 280 ngày mỗi ngày đọc một trang}',\n",
              " '{vô cùng tàn nhẫn vô cùng yêu thương tái bản 2017}',\n",
              " '{đọc vị mọi vấn đề của trẻ tái bản 2018}',\n",
              " '{ăn dặm kiểu nhật tái bản 2018}',\n",
              " '{người mẹ tốt hơn là người thầy tốt tái bản 2015}',\n",
              " '{combo nuôi con không phải cuộc chiến bộ 4 cuốn}']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "45MNis1mj7bG",
        "outputId": "6727239b-2f9c-4555-c7a1-cd687f7710c9"
      },
      "source": [
        "# augment\n",
        "text = []\n",
        "for line in lines:\n",
        "  line = [line]*10\n",
        "  text.extend(line)\n",
        "random.shuffle(text)\n",
        "text = \"\".join(text)\n",
        "text[:500]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{bí quyết chinh phục điểm cao toán 6 tập 2}{đất rừng phương nam}{thiết kế bài soạn môn toán phát triển năng lực học sinh tiểu học}{cuốn sách đầu tiên của bé về động vật}{lực cơ học của tình yêu bộ 2 tập tặng kèm postcard có trích dẫn truyện}{thành phố thông minh nền tảng nguyên lý và ứng dụng}{rich habits poor habits sự khác biệt giữa người giàu và người nghèo}{dám nghĩ lớn tái bản 2019}{nhà giả kim}{ảnh đế bộ 2 tập}{tôi thấy hoa vàng trên cỏ xanh top những cuốn sách bán chạy của nguyễn nhật ánh'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8OONYs7FsC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05c66bd-182e-4af2-b1c0-a1b37d1c93fe"
      },
      "source": [
        "#Create vocabulary\n",
        "vocab = sorted(set(text))\n",
        "print(\"vocab len:\", len(vocab))\n",
        "#create an index for each character\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "conver_text_to_int = np.array([char2idx[char] for char in text])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab len: 106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvtTsAATGmOu"
      },
      "source": [
        "#convert the text vector into a stream of character indices.\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(conver_text_to_int)\n",
        "#Each sample has 100 chars\n",
        "seq_length = 100\n",
        "#convert char to sentences of 100 chars\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "#split into input and targer, each length 100\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yletLjrJG9-f"
      },
      "source": [
        "#shuffle and batch samples\n",
        "BATCH_SIZE = 30\n",
        "dataset = dataset.shuffle(10000).batch(BATCH_SIZE,drop_remainder=True)\n",
        "embedding_dim = 256\n",
        "rnn_units=1024"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3si9vffdHEJq"
      },
      "source": [
        "# **3. Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKojRll5HHm8"
      },
      "source": [
        "def build_model(embedding_dim,rnn_units,batch_size,vocab_size):\n",
        "  model = tf.keras.Sequential(\n",
        "  [tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "     tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def build_lstm_model(embedding_dim,rnn_units,batch_size,vocab_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n",
        "     tf.keras.layers.LSTM(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "  return model\n",
        "\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lmbgr6gsuzS"
      },
      "source": [
        "## **3.1. GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "9Wa-Vk85HleF",
        "outputId": "91be5f70-7454-4b6e-82bd-84084e1e7f5c"
      },
      "source": [
        "#Train model GRU layer\n",
        "model = build_model(embedding_dim,rnn_units,BATCH_SIZE,len(vocab))\n",
        "model.summary()\n",
        "model_save_dir = '/content/drive/MyDrive/LSTM/RNN'\n",
        "checkpoint_prefix = os.path.join(model_save_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "history = model.fit(dataset, epochs=30,callbacks=[checkpoint_callback, early_stop_callback])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_wrapper_1                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mEmbeddingWrapper\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_13 (\u001b[38;5;33mGRU\u001b[0m)                         │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_wrapper_1                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EmbeddingWrapper</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                         │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"gru_13\" is incompatible with the layer: expected ndim=3, found ndim=5. Full shape received: (30, 100, 30, 100, 256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-ffb800fb9d44>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mearly_stop_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_last_axis_squeeze\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"gru_13\" is incompatible with the layer: expected ndim=3, found ndim=5. Full shape received: (30, 100, 30, 100, 256)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cpTXQTQksJZG",
        "outputId": "95f3e16f-fb50-4e0c-c3ea-c82f2fd3852e"
      },
      "source": [
        "model_save_dir = '/content/drive/MyDrive/LSTM/RNN'\n",
        "generate_model = build_model(embedding_dim,rnn_units,1,len(vocab))\n",
        "generate_model.load_weights(tf.train.latest_checkpoint(model_save_dir))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [1, None]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7472dde65362>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_save_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/LSTM/RNN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-918a4055c80a>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(embedding_dim, rnn_units, batch_size, vocab_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   model = tf.keras.Sequential(\n\u001b[0;32m----> 3\u001b[0;31m   [tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n\u001b[0m\u001b[1;32m      4\u001b[0m      tf.keras.layers.GRU(rnn_units,\n\u001b[1;32m      5\u001b[0m                             \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, weights, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;34m\"Argument `input_length` is deprecated. Just remove it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             )\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_shape_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34m\"Unrecognized keyword arguments \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"passed to {self.__class__.__name__}: {kwargs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [1, None]}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kpgG0Ss_W0"
      },
      "source": [
        "## **3.2. LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "yRmNoBG5wbdW",
        "outputId": "8fa399c5-b224-4190-e4b1-1e9010dd3a90"
      },
      "source": [
        "#Train model LSTM\n",
        "model_lstm = build_lstm_model(embedding_dim,rnn_units,BATCH_SIZE,len(vocab))\n",
        "model_lstm.summary()\n",
        "#train model\n",
        "#add checkpoint save\n",
        "model_save_dir = '/content/drive/My Drive/ML/RNN/checkpointlstm1'\n",
        "checkpoint_prefix = os.path.join(model_save_dir, \"ckpt_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
        "model_lstm.compile(optimizer='adam', loss=loss)\n",
        "model_lstm.fit(dataset, epochs=30,callbacks=[checkpoint_callback, early_stop_callback])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [64, None]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-1ae151c3eb73>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train model LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#add checkpoint save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-918a4055c80a>\u001b[0m in \u001b[0;36mbuild_lstm_model\u001b[0;34m(embedding_dim, rnn_units, batch_size, vocab_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size,embedding_dim,batch_input_shape=[batch_size,None]),\n\u001b[0m\u001b[1;32m     14\u001b[0m      tf.keras.layers.LSTM(rnn_units,\n\u001b[1;32m     15\u001b[0m                             \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, weights, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;34m\"Argument `input_length` is deprecated. Just remove it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             )\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_shape_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34m\"Unrecognized keyword arguments \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"passed to {self.__class__.__name__}: {kwargs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'batch_input_shape': [64, None]}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3uXRI5atll0",
        "outputId": "e1a63b80-3a12-492e-fd3a-2406ea75f4a8"
      },
      "source": [
        "model_save_dir = '/content/drive/My Drive/ML/RNN/checkpointlstm1'\n",
        "generate_model_lstm = build_lstm_model(embedding_dim,rnn_units,1,len(vocab))\n",
        "generate_model_lstm.load_weights(tf.train.latest_checkpoint(model_save_dir)).expect_partial()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fae4011d358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-13Y5D3att4h"
      },
      "source": [
        "# **4. Text Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hF9RIvhsHkKN"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "    num_generate = 100\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    print(input_eval.shape)\n",
        "    text_generated = []\n",
        "    model.reset_states() #delete hidden state\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)# drop batch dimensionality\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        prob = special.softmax(predictions[-1])\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        if idx2char[predicted_id] == \"}\":\n",
        "          text_generated = text_generated[:-1]\n",
        "          break\n",
        "        if max(prob) < 0.2:\n",
        "          break\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uNdE7rKuLNk"
      },
      "source": [
        "## **4.1. GRU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uocah2v3dj-",
        "outputId": "90ba3c81-61ed-4631-b89f-53baccc8c6ae"
      },
      "source": [
        "#Build new model to generate\n",
        "result_of_gru_char = generate_text(generate_model, start_string=u\"dế mèn phiê\")\n",
        "print(result_of_gru_char)\n",
        "result_of_gru_char = generate_text(generate_model, start_string=u\"nhà kh\")\n",
        "print(result_of_gru_char)\n",
        "result_of_gru_char = generate_text(generate_model, start_string=u\"sách tập làm v\")\n",
        "print(result_of_gru_char)\n",
        "result_of_gru_char = generate_text(generate_model, start_string=u\"thanh lọ\")\n",
        "print(result_of_gru_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 11)\n",
            "dế mèn phiêu lưu ký tái nhà ăn cơm học\n",
            "(1, 6)\n",
            "nhà khi đúng b\n",
            "(1, 14)\n",
            "sách tập làm việc nhà thuật x\n",
            "(1, 8)\n",
            "thanh lọc ốc diệu của philập tư duy vệ sách mẹ nhà trường chứng khoán nhật kỳ lực chi kháng kèm s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve7EofiSuTF0"
      },
      "source": [
        "## **4.2. LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKsJW6U3znTF",
        "outputId": "19b36df6-a68c-4298-a6b3-84a80af665f2"
      },
      "source": [
        "result_of_gru_char = generate_text(generate_model_lstm, start_string=u\"dế mèn phiê\")\n",
        "print(result_of_gru_char)\n",
        "result_of_gru_char = generate_text(generate_model_lstm, start_string=u\"nhà kh\")\n",
        "print(result_of_gru_char)\n",
        "result_of_gru_char = generate_text(generate_model_lstm, start_string=u\"sách tập làm v\")\n",
        "print(result_of_gru_char)\n",
        "result_of_gru_char = generate_text(generate_model_lstm, start_string=u\"thanh lọ\")\n",
        "print(result_of_gru_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 11)\n",
            "dế mèn phiêu lưu ký khi những điều lấp lánh được gọi tên tái bản\n",
            "(1, 6)\n",
            "nhà khoa học\n",
            "(1, 14)\n",
            "sách tập làm văn\n",
            "(1, 8)\n",
            "thanh lọc cơ thể và giảm cân\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoiXPIkFuvUT"
      },
      "source": [
        "# **5. Spelling correction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obAvlTrwmGhG"
      },
      "source": [
        "## **5.1. Left to Right without Lookahead**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10q2fX2y0Ry"
      },
      "source": [
        "def correct_text(model, text, begin=7, threshold=0.001):\n",
        "  correct = text[:begin]\n",
        "  misspell = text[:begin]\n",
        "  misspell_detected = False\n",
        "\n",
        "  print(\"Assume the first \" + str(begin) + \" chars are correct\")\n",
        "  seq = [char2idx[c] for c in text[:begin]]\n",
        "  seq = tf.expand_dims(seq, 0)\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(begin, len(text)):\n",
        "    predictions = model(seq)\n",
        "    predictions = tf.squeeze(predictions, 0)[-1]\n",
        "    probs = special.softmax(predictions)\n",
        "\n",
        "    if probs[char2idx[text[i]]] < threshold:\n",
        "      misspell_detected = True\n",
        "      misspell += \"(\" + text[i] + \")\"\n",
        "      corrected_char = tf.math.top_k(predictions).indices[0]\n",
        "      correct += idx2char[corrected_char]\n",
        "      print(f\"{misspell} --> {correct}\")\n",
        "    else:\n",
        "      misspell += text[i]\n",
        "      correct += text[i]\n",
        "\n",
        "    seq = tf.expand_dims([char2idx[correct[-1]]], 0)\n",
        "\n",
        "  if not misspell_detected:\n",
        "    misspell = \"\"\n",
        "\n",
        "  print(\"misspell: \", misspell)\n",
        "  print(\"correct: \", correct)\n",
        "  print()\n",
        "  return correct, misspell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXCFlmCGm7r4",
        "outputId": "27fb4d6d-8540-4763-e80d-2d7bfb120083"
      },
      "source": [
        "# Good cases\n",
        "correct_text(generate_model_lstm, \"dế mèn phieu lưu ký táo bản\")\n",
        "correct_text(generate_model_lstm, \"dòng suoi nguồn thịnh vuong\")\n",
        "correct_text(generate_model_lstm, \"dòng suối nguồn thịnh vượng\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assume the first 7 chars are correct\n",
            "dế mèn phi(e) --> dế mèn phiê\n",
            "dế mèn phi(e)u lưu ký tá(o) --> dế mèn phiêu lưu ký tái\n",
            "misspell:  dế mèn phi(e)u lưu ký tá(o) bản\n",
            "correct:  dế mèn phiêu lưu ký tái bản\n",
            "\n",
            "Assume the first 7 chars are correct\n",
            "dòng su(o) --> dòng suố\n",
            "dòng su(o)i nguồn thịnh v(u) --> dòng suối nguồn thịnh vư\n",
            "dòng su(o)i nguồn thịnh v(u)(o) --> dòng suối nguồn thịnh vượ\n",
            "misspell:  dòng su(o)i nguồn thịnh v(u)(o)ng\n",
            "correct:  dòng suối nguồn thịnh vượng\n",
            "\n",
            "Assume the first 7 chars are correct\n",
            "misspell:  \n",
            "correct:  dòng suối nguồn thịnh vượng\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDsZrqKlmRTc",
        "outputId": "1932a9b3-4c1c-4a61-8a30-a3d502a16fc3"
      },
      "source": [
        "# bad case\n",
        "correct_text(generate_model_lstm, \"dòng suối nnguồn thịnh vượng\")\n",
        "correct_text(generate_model_lstm, \"dòng suối naaguồn thịnh vượng\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assume the first 7 chars are correct\n",
            "dòng suối n(n) --> dòng suối ng\n",
            "dòng suối n(n)(g) --> dòng suối ngu\n",
            "dòng suối n(n)(g)(u) --> dòng suối nguồ\n",
            "dòng suối n(n)(g)(u)(ồ) --> dòng suối nguồn\n",
            "dòng suối n(n)(g)(u)(ồ)(n) --> dòng suối nguồn \n",
            "dòng suối n(n)(g)(u)(ồ)(n)( ) --> dòng suối nguồn t\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t) --> dòng suối nguồn th\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h) --> dòng suối nguồn thị\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị) --> dòng suối nguồn thịn\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n) --> dòng suối nguồn thịnh\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h) --> dòng suối nguồn thịnh \n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( ) --> dòng suối nguồn thịnh v\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( )(v) --> dòng suối nguồn thịnh vư\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( )(v)(ư) --> dòng suối nguồn thịnh vượ\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( )(v)(ư)(ợ) --> dòng suối nguồn thịnh vượn\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( )(v)(ư)(ợ)(n) --> dòng suối nguồn thịnh vượng\n",
            "dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( )(v)(ư)(ợ)(n)(g) --> dòng suối nguồn thịnh vượng \n",
            "misspell:  dòng suối n(n)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)(h)( )(v)(ư)(ợ)(n)(g)\n",
            "correct:  dòng suối nguồn thịnh vượng \n",
            "\n",
            "Assume the first 7 chars are correct\n",
            "dòng suối n(a) --> dòng suối ng\n",
            "dòng suối n(a)(a) --> dòng suối ngu\n",
            "dòng suối n(a)(a)(g) --> dòng suối nguồ\n",
            "dòng suối n(a)(a)(g)(u) --> dòng suối nguồn\n",
            "dòng suối n(a)(a)(g)(u)(ồ) --> dòng suối nguồn \n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n) --> dòng suối nguồn t\n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( ) --> dòng suối nguồn th\n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t) --> dòng suối nguồn thị\n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t)(h) --> dòng suối nguồn thịn\n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t)(h)(ị) --> dòng suối nguồn thịnh\n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t)(h)(ị)(n) --> dòng suối nguồn thịnh \n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)h( ) --> dòng suối nguồn thịnh hư\n",
            "dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)h( )(v) --> dòng suối nguồn thịnh hư \n",
            "misspell:  dòng suối n(a)(a)(g)(u)(ồ)(n)( )(t)(h)(ị)(n)h( )(v)ượng\n",
            "correct:  dòng suối nguồn thịnh hư ượng\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5GP1yAOmoFe"
      },
      "source": [
        "## **5.2 Left to Right with Lookahead**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIHtQlCpeKUu",
        "outputId": "b81d2b0f-c02a-4d26-9620-a10e5dc03237"
      },
      "source": [
        "def get_prob_of_text(model, text, begin):\n",
        "  prob = 1\n",
        "  if begin >= len(text):\n",
        "    return prob\n",
        "\n",
        "  seq = [char2idx[c] for c in text]\n",
        "  model.reset_states()\n",
        "  predictions = model(tf.expand_dims(seq, 0))\n",
        "  predictions = tf.squeeze(predictions, 0)\n",
        "  for i in range(begin, len(text)):\n",
        "    probs = special.softmax(predictions[i-1])\n",
        "    prob *= probs[char2idx[text[i]]]\n",
        "\n",
        "  return prob\n",
        "\n",
        "test = [\"dế mèn phiêu lưu ký\", \"dế mèn phiêu lu ký\", \"dế mèn phiêu ưu ký\"]\n",
        "for t in test:\n",
        "  print(t, \":\", get_prob_of_text(generate_model_lstm, t, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dế mèn phiêu lưu ký : 0.8976038268369191\n",
            "dế mèn phiêu lu ký : 3.850490837557988e-06\n",
            "dế mèn phiêu ưu ký : 2.5335562370135195e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWkVew0HEScH"
      },
      "source": [
        "def correct_text_lookahead(model, text, begin=7, threshold=0.001):\n",
        "  correct = text[:begin]\n",
        "  misspell = text[:begin]\n",
        "  misspell_detected = False\n",
        "\n",
        "  print(\"Assume the first \" + str(begin) + \" chars are correct\")\n",
        "\n",
        "  seq = [char2idx[c] for c in text[:begin]]\n",
        "  for i in range(begin, len(text)):\n",
        "    model.reset_states()\n",
        "    predictions = model(tf.expand_dims(seq, 0))\n",
        "    predictions = tf.squeeze(predictions, 0)[-1]\n",
        "    probs = special.softmax(predictions)\n",
        "\n",
        "    if probs[char2idx[text[i]]] < threshold:\n",
        "      misspell_detected = True\n",
        "      top_k_next_chars = tf.math.top_k(probs, k=3).indices\n",
        "      options = [correct + idx2char[c] + text[i+1:] for c in top_k_next_chars] # replace text[i]\n",
        "      options.append(correct + text[i+1:]) # remove text[i]\n",
        "      options_probs = [get_prob_of_text(model, option, len(correct)) for option in options]\n",
        "      chosen = np.argmax(options_probs)\n",
        "      misspell += \"(\" + text[i] + \")\"\n",
        "      if chosen != len(options)-1:\n",
        "        corrected_char = top_k_next_chars[chosen]\n",
        "        correct += idx2char[corrected_char]\n",
        "      print(f\"{misspell} --> {correct}\")\n",
        "    else:\n",
        "      misspell += text[i]\n",
        "      correct += text[i]\n",
        "\n",
        "    seq.append(char2idx[correct[-1]])\n",
        "\n",
        "  if not misspell_detected:\n",
        "    misspell = \"\"\n",
        "\n",
        "  print(f\"Misspell: {misspell}\\nCorrect: {correct}\\n\")\n",
        "  return correct, misspell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36RIfCWgRKwf",
        "outputId": "35aa673b-c70e-48f5-e0fe-3e2fe17c5c85"
      },
      "source": [
        "correct_text_lookahead(generate_model_lstm, \"dế mèn phieu lưu ký táo bản\")\n",
        "correct_text_lookahead(generate_model_lstm, \"dòng suoi nguồn thịnh vuợng\")\n",
        "correct_text_lookahead(generate_model_lstm, \"dòng suối nguồn thịnh vượng\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assume the first 7 chars are correct\n",
            "dế mèn phi(e) --> dế mèn phiê\n",
            "dế mèn phi(e)u lưu ký tá(o) --> dế mèn phiêu lưu ký tái\n",
            "Misspell: dế mèn phi(e)u lưu ký tá(o) bản\n",
            "Correct: dế mèn phiêu lưu ký tái bản\n",
            "\n",
            "Assume the first 7 chars are correct\n",
            "dòng su(o) --> dòng suố\n",
            "dòng su(o)i nguồn thịnh v(u) --> dòng suối nguồn thịnh vư\n",
            "Misspell: dòng su(o)i nguồn thịnh v(u)ợng\n",
            "Correct: dòng suối nguồn thịnh vượng\n",
            "\n",
            "Assume the first 7 chars are correct\n",
            "Misspell: \n",
            "Correct: dòng suối nguồn thịnh vượng\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('dòng suối nguồn thịnh vượng', '')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iRRXJh-m5Hx",
        "outputId": "21232fc0-52d4-4699-ddbb-4e36c729781b"
      },
      "source": [
        "correct_text_lookahead(generate_model_lstm, \"dòng suối nnguồn thịnh vượng\")\n",
        "correct_text_lookahead(generate_model_lstm, \"dòng suối naaguồn thịnh vượng\")\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assume the first 7 chars are correct\n",
            "dòng suối n(n) --> dòng suối n\n",
            "Misspell: dòng suối n(n)guồn thịnh vượng\n",
            "Correct: dòng suối nguồn thịnh vượng\n",
            "\n",
            "Assume the first 7 chars are correct\n",
            "dòng suối n(a) --> dòng suối n\n",
            "dòng suối n(a)(a) --> dòng suối n\n",
            "Misspell: dòng suối n(a)(a)guồn thịnh vượng\n",
            "Correct: dòng suối nguồn thịnh vượng\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeLBXetxnZvS"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}